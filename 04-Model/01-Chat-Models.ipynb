{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee483b3",
   "metadata": {},
   "source": [
    "# 다양한 LLM 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf28f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd6b300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Models\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH04-Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550920c-09d8-48b3-be2f-b36362c37989",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "### 개요\n",
    "OpenAI는 채팅 전용 Large Language Model (LLM)을 제공합니다. 이 모델을 생성할 때 다양한 옵션을 지정할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향을 미칩니다.\n",
    "\n",
    "### 옵션 상세 설명\n",
    "\n",
    "`temperature`\n",
    "\n",
    "- 샘플링 온도를 설정하는 옵션입니다. 값은 0과 2 사이에서 선택할 수 있습니다. 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "- 채팅 완성에서 생성할 토큰의 최대 개수를 지정합니다. 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어합니다.\n",
    "\n",
    "`model_name`\n",
    "\n",
    "- 적용 가능한 모델을 선택하는 옵션입니다. 더 자세한 정보는 [OpenAI 모델 문서](https://platform.openai.com/docs/models)에서 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "**모델 스펙**\n",
    "\n",
    "- 링크: https://platform.openai.com/docs/models/gpt-4o\n",
    "\n",
    "| 모델명 | 설명 | 컨텍스트 길이 | 최대 출력 토큰 | 학습 데이터 |\n",
    "|--------|------|---------------|-----------------|-------------|\n",
    "| gpt-4o | GPT-4 터보보다 저렴하고 빠른 최신 다중모드 플래그십 모델 | 128,000 토큰 | 4,096 토큰 | 2023년 10월까지 |\n",
    "| gpt-4-turbo | 최신 GPT-4 터보 모델. 비전 기능, JSON 모드, 기능 호출 지원 | 128,000 토큰 | 4,096 토큰 | 2023년 12월까지 |\n",
    "| gpt-4o-mini | GPT-3.5 터보보다 더 우수한 성능의 작은 모델 | 128,000 토큰 | 16,384 토큰 | 2023년 10월까지 |\n",
    "| o1-preview | 다양한 도메인의 어려운 문제 해결을 위한 추론 모델 | 128,000 토큰 | 32,768 토큰 | 2023년 10월까지 |\n",
    "| o1-mini | 코딩, 수학, 과학에 특화된 빠른 추론 모델 | 128,000 토큰 | 65,536 토큰 | 2023년 10월까지 |\n",
    "| gpt-4o-realtime | 실시간 API용 오디오 및 텍스트 입력 처리 모델 (베타) | 128,000 토큰 | 4,096 토큰 | 2023년 10월까지 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc161c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 객체를 생성합니다.\n",
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aa412",
   "metadata": {},
   "source": [
    "# Anthropic\n",
    "\n",
    "Anthropic은 인공지능(AI) 안전성과 연구에 중점을 둔 미국의 스타트업 기업입니다. 주요 정보는 다음과 같습니다:\n",
    "\n",
    "- **설립 연도**: 2021년\n",
    "- **위치**: 미국 샌프란시스코\n",
    "- **창립자**: OpenAI 출신 직원들 (Daniela Amodei와 Dario Amodei 등)\n",
    "- **기업 형태**: 공익기업(Public Benefit Corporation)으로 등록\n",
    "\n",
    "## Claude\n",
    "\n",
    "Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군입니다. \n",
    "\n",
    "- **API 키 발급**: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)\n",
    "- **모델 리스트**: [https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "\n",
    "\n",
    "![](images/anthropic-20241023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c7a79",
   "metadata": {},
   "source": [
    "| model_name        | model                              | Anthropic API                                | AWS Bedrock                                   | GCP Vertex AI                      |\n",
    "|------------------|------------------------------------|--------------------------------------------|----------------------------------------------|-----------------------------------|\n",
    "| Claude 3.5 Opus  | 연말 출시 예정                         | 연말 출시 예정                                | 연말 출시 예정                                   | 연말 출시 예정                        |\n",
    "| Claude 3.5 Sonnet| claude-3-5-sonnet-20241022         | claude-3-5-sonnet-20241022                 | anthropic.claude-3-5-sonnet-20241022-v2:0     | claude-3-5-sonnet-v2@20241022      |\n",
    "| Claude 3.5 Haiku | 연말 출시 예정                         | 연말 출시 예정                                | 연말 출시 예정                                   | 연말 출시 예정                        |\n",
    "| Claude 3 Opus    | claude-3-opus-20240229             | claude-3-opus-20240229                     | anthropic.claude-3-opus-20240229-v1:0         | claude-3-opus@20240229             |\n",
    "| Claude 3 Sonnet  | claude-3-sonnet-20240229           | claude-3-sonnet-20240229                   | anthropic.claude-3-sonnet-20240229-v1:0       | claude-3-sonnet@20240229           |\n",
    "| Claude 3 Haiku   | claude-3-haiku-20240307            | claude-3-haiku-20240307                    | anthropic.claude-3-haiku-20240307-v1:0        | claude-3-haiku@20240307            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b38108",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m answer \u001b[39m=\u001b[39m anthropic\u001b[39m.\u001b[39mstream(\u001b[39m\"\u001b[39m\u001b[39m사랑이 뭔가요?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# 답변 출력\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m stream_response(answer)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/langchain_teddynote/messages.py:32\u001b[0m, in \u001b[0;36mstream_response\u001b[0;34m(response, return_output)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mAI 모델로부터의 응답을 스트리밍하여 각 청크를 처리하면서 출력합니다.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m- str: `return_output`이 True인 경우, 연결된 응답 문자열입니다. 그렇지 않으면, 아무것도 반환되지 않습니다.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m response:\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(token, AIMessageChunk):\n\u001b[1;32m     34\u001b[0m         answer \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m token\u001b[39m.\u001b[39;49mcontent\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:420\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(\n\u001b[1;32m    415\u001b[0m         e,\n\u001b[1;32m    416\u001b[0m         response\u001b[39m=\u001b[39mLLMResult(\n\u001b[1;32m    417\u001b[0m             generations\u001b[39m=\u001b[39m[[generation]] \u001b[39mif\u001b[39;00m generation \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    418\u001b[0m         ),\n\u001b[1;32m    419\u001b[0m     )\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    421\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_end(LLMResult(generations\u001b[39m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:400\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrate_limiter\u001b[39m.\u001b[39macquire(blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    399\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream(messages, stop\u001b[39m=\u001b[39;49mstop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    401\u001b[0m         \u001b[39mif\u001b[39;49;00m chunk\u001b[39m.\u001b[39;49mmessage\u001b[39m.\u001b[39;49mid \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    402\u001b[0m             chunk\u001b[39m.\u001b[39;49mmessage\u001b[39m.\u001b[39;49mid \u001b[39m=\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrun-\u001b[39;49m\u001b[39m{\u001b[39;49;00mrun_manager\u001b[39m.\u001b[39;49mrun_id\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:720\u001b[0m, in \u001b[0;36mChatAnthropic._stream\u001b[0;34m(self, messages, stop, run_manager, stream_usage, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    719\u001b[0m payload \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_request_payload(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 720\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mmessages\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpayload)\n\u001b[1;32m    721\u001b[0m coerce_content_to_string \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m _tools_in_params(payload)\n\u001b[1;32m    722\u001b[0m \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/anthropic/resources/messages.py:888\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    882\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    883\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is deprecated and will reach end-of-life on \u001b[39m\u001b[39m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    884\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    885\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    889\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/v1/messages\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    890\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    891\u001b[0m         {\n\u001b[1;32m    892\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    893\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    895\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    896\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop_sequences,\n\u001b[1;32m    897\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    898\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m: system,\n\u001b[1;32m    899\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    900\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_k,\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    904\u001b[0m         },\n\u001b[1;32m    905\u001b[0m         message_create_params\u001b[39m.\u001b[39;49mMessageCreateParams,\n\u001b[1;32m    906\u001b[0m     ),\n\u001b[1;32m    907\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    908\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    909\u001b[0m     ),\n\u001b[1;32m    910\u001b[0m     cast_to\u001b[39m=\u001b[39;49mMessage,\n\u001b[1;32m    911\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    912\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[RawMessageStreamEvent],\n\u001b[1;32m    913\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/anthropic/_base_client.py:1279\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1267\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1275\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1276\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1277\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1278\u001b[0m     )\n\u001b[0;32m-> 1279\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/anthropic/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    957\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    958\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    959\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    960\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    961\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    962\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/anthropic/_base_client.py:1060\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1059\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1063\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1064\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1069\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# ChatAnthropic 객체를 생성합니다.\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = anthropic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea6a72",
   "metadata": {},
   "source": [
    "# Cohere\n",
    "\n",
    "Cohere는 기업용 인공지능 솔루션을 제공하는 선도적인 AI 기업으로, 대규모 언어 모델(LLM)을 개발하여 기업들이 AI 기술을 쉽게 도입하고 활용할 수 있도록 돕고 있습니다.\n",
    "\n",
    "## Cohere 개요\n",
    "\n",
    "- **설립연도**: 2020년\n",
    "- **주요 투자자**: Inovia Capital, NVIDIA, Oracle, Salesforce Ventures\n",
    "- **시리즈 C 펀딩**: 2억 7000만 달러 유치\n",
    "- **기업 미션**: 기업용 AI 플랫폼 제공\n",
    "\n",
    "## 주요 제품\n",
    "\n",
    "### Command R+\n",
    "\n",
    "Command R+는 기업용으로 최적화된 Cohere의 최신 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **긴 컨텍스트 윈도우**: 128k 토큰 지원\n",
    "- **고급 RAG 기능**: 검색 강화 생성 기능 제공\n",
    "- **다국어 지원**: 10개 주요 비즈니스 언어 지원\n",
    "- **자동화 도구 사용 기능**: 복잡한 비즈니스 프로세스 자동화\n",
    "\n",
    "### Aya\n",
    "\n",
    "Aya는 Cohere의 비영리 연구소인 Cohere for AI에서 개발한 오픈소스 다국어 LLM입니다. \n",
    "\n",
    "#### 주요 특징\n",
    "\n",
    "- **언어 지원**: 101개 언어 지원 (기존 오픈소스 모델의 두 배 이상)\n",
    "- **훈련 데이터셋**: 5억 1300만 개의 데이터 포인트 포함하는 대규모 다국어 훈련 데이터셋 공개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# ChatCohere 객체를 생성합니다.\n",
    "cohere = ChatCohere(temperature=0)\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = cohere.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640b099",
   "metadata": {},
   "source": [
    "# Upstage\n",
    "\n",
    "Upstage는 인공지능(AI) 기술, 특히 대규모 언어 모델(LLM)과 문서 AI 분야에 특화된 국내 스타트업입니다.\n",
    "\n",
    "## 주요 제품 및 기술\n",
    "\n",
    "### Solar LLM\n",
    "- **주요 특징**: Upstage의 주력 대규모 언어 모델로, 빠른 성능과 비용 효율성으로 주목받고 있습니다.\n",
    "- **기술적 접근**: Depth-Up Scaling (DUS) 기술을 적용하여 성능을 극대화합니다.\n",
    "- **플랫폼 통합**: Amazon SageMaker JumpStart 등 다양한 플랫폼을 통해 API로 통합 제공됩니다.\n",
    "\n",
    "### Document AI Pack\n",
    "- **기능**: OCR 기술을 기반으로 한 문서 처리 솔루션으로, 복잡한 문서에서 필요한 내용을 정확히 추출하고 디지털화합니다.\n",
    "\n",
    "### AskUp Seargest\n",
    "- **특징**: 개인화된 검색 및 추천 서비스를 제공하며, 기존의 ChatGPT 통합 무료 챗봇 AskUp의 업그레이드 버전입니다.\n",
    "\n",
    "## API 키 발급\n",
    "API 키 발급은 [여기](https://console.upstage.ai/api-keys)에서 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10919710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "url = \"https://api.upstage.ai/v1/solar/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f'Bearer {os.environ.get(\"UPSTAGE_API_KEY\")}',\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe how we plan to leverage Upstage products to achieve your mission of AGI for Work.\",\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# 스트림 응답 처리\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            if decoded_line.startswith(\"data: \"):\n",
    "                json_data = json.loads(decoded_line[6:])\n",
    "                print(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON 디코딩 오류: {decoded_line}\")\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSTAGE API KEY 설정\n",
    "import os\n",
    "\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = \"이곳에 API KEY를 입력하세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc739c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# ChatUpstage 객체를 생성합니다.\n",
    "upstage = ChatUpstage(model=\"solar-pro\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = upstage.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41206a27",
   "metadata": {},
   "source": [
    "## Xionic\n",
    "\n",
    "사이오닉에이아이(Sionic AI)는 대한민국의 유망한 인공지능 스타트업으로, 기업용 생성형 AI 솔루션을 개발하고 있습니다. 다음은 이 회사에 대한 주요 정보입니다:\n",
    "\n",
    "### 주요 제품\n",
    "\n",
    "1. **STORM Platform**: 기업이 생성형 AI를 기술적 고민 없이 바로 적용할 수 있도록 하는 플랫폼\n",
    "2. **STORM Answer**: 기업에 최적화된 생성형 AI 솔루션으로 비즈니스 생산성 향상을 목표로 함\n",
    "3. **Xionic**: 상업적 이용이 가능한 라이센스의 한국어 AI 모델\n",
    "\n",
    "상업적 이용이 가능한 라이센스의 한국어 모델\n",
    "\n",
    "- 링크: https://github.com/sionic-ai/xionic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f39d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mconnect_tcp\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_backend\u001b[39m.\u001b[39;49mconnect_tcp(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    125\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[39m=\u001b[39;49m socket\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[39m=\u001b[39;49msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    994\u001b[0m         request,\n\u001b[1;32m    995\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    996\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    927\u001b[0m     request,\n\u001b[1;32m    928\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    929\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    930\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    931\u001b[0m )\n\u001b[1;32m    932\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    955\u001b[0m         request,\n\u001b[1;32m    956\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    957\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    992\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1029\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:235\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[0;32m--> 235\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[1;32m    236\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mYou are an AI assistant. You will be given a task. You must generate a detailed and long answer in Korean.\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     13\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m사랑이 뭔가요?\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m \u001b[39m# 스트리밍 응답 생성\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     18\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mxionic-1-72b-20240610\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     20\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m  \u001b[39m# 스트리밍 모드 활성화\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39m# 응답 출력\u001b[39;00m\n\u001b[1;32m     24\u001b[0m stream_response(response)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    830\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    831\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    832\u001b[0m             {\n\u001b[1;32m    833\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    834\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    835\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    836\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    837\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    838\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    839\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    840\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    843\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    844\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    845\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    846\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    847\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    848\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    849\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    850\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    851\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    852\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    853\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    854\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    855\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    856\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    857\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    858\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    859\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    860\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    861\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    862\u001b[0m             },\n\u001b[1;32m    863\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    864\u001b[0m         ),\n\u001b[1;32m    865\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    866\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    867\u001b[0m         ),\n\u001b[1;32m    868\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    869\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    870\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    871\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    958\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    961\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    962\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1017\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1014\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1018\u001b[0m         input_options,\n\u001b[1;32m   1019\u001b[0m         cast_to,\n\u001b[1;32m   1020\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1021\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1022\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1023\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1027\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1017\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1014\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1018\u001b[0m         input_options,\n\u001b[1;32m   1019\u001b[0m         cast_to,\n\u001b[1;32m   1020\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1021\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1022\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1023\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1027\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-_2_6u2s--py3.11/lib/python3.11/site-packages/openai/_base_client.py:1027\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m   1018\u001b[0m             input_options,\n\u001b[1;32m   1019\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             response_headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1024\u001b[0m         )\n\u001b[1;32m   1026\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1027\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1030\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHTTP Response: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     request\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     response\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1036\u001b[0m )\n\u001b[1;32m   1037\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mrequest_id: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mx-request-id\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 2024. 11. 21 업데이트 코드\n",
    "xionic = ChatOpenAI(\n",
    "    model_name=\"xionic-1-72b-20240610\",\n",
    "    base_url=\"https://sionic.chat/v1/\",\n",
    "    api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = xionic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3eb61",
   "metadata": {},
   "source": [
    "# LogicKor\n",
    "\n",
    "LogicKor는 한국어 언어 모델의 다분야 사고력을 평가하기 위해 만들어진 벤치마크 리더보드입니다.\n",
    "\n",
    "## 목적\n",
    "\n",
    "한국어 언어 모델의 다양한 분야에서의 사고력을 측정하는 벤치마크\n",
    "\n",
    "## 평가 영역\n",
    "\n",
    "- 한국어 추론\n",
    "- 수학\n",
    "- 글쓰기\n",
    "- 코딩\n",
    "- 이해력\n",
    "\n",
    "## 주요 특징\n",
    "\n",
    "1. **다양한 모델 평가**: 국내외 다양한 언어 모델들의 성능을 비교할 수 있음\n",
    "2. **객관적 성능 측정**: 모델의 실제 성능을 다각도로 평가하여 객관적인 지표 제공\n",
    "3. **오픈 소스**: 누구나 접근하고 결과를 확인할 수 있는 오픈 플랫폼\n",
    "\n",
    "LogicKor 리더보드는 한국어 AI 모델의 발전을 위한 중요한 도구로 자리잡고 있으며, 지속적인 개선과 발전이 기대되고 있습니다.\n",
    "\n",
    "- 링크: [LogicKor 리더보드](https://lk.instruct.kr/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
